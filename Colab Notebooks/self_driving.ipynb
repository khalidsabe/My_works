{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Untitled0.ipynb","provenance":[],"authorship_tag":"ABX9TyPyGwcHS5zWRVFTv1VNRANp"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"kYhUsz0WX8qo"},"outputs":[],"source":["from __future__ import print_function\n","from __future__ import division\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.nn import Parameter\n","import math\n","\n","\n","class ArcMarginProduct(nn.Module):\n","    r\"\"\"Implement of large margin arc distance: :\n","        Args:\n","            in_features: size of each input sample\n","            out_features: size of each output sample\n","            s: norm of input feature\n","            m: margin\n","\n","            cos(theta + m)\n","        \"\"\"\n","    def __init__(self, in_features, out_features, s=30.0, m=0.50, easy_margin=False):\n","        super(ArcMarginProduct, self).__init__()\n","        self.in_features = in_features\n","        self.out_features = out_features\n","        self.s = s\n","        self.m = m\n","        self.weight = Parameter(torch.FloatTensor(out_features, in_features))\n","        nn.init.xavier_uniform_(self.weight)\n","\n","        self.easy_margin = easy_margin\n","        self.cos_m = math.cos(m)\n","        self.sin_m = math.sin(m)\n","        self.th = math.cos(math.pi - m)\n","        self.mm = math.sin(math.pi - m) * m\n","\n","    def forward(self, input, label):\n","        # --------------------------- cos(theta) & phi(theta) ---------------------------\n","        cosine = F.linear(F.normalize(input), F.normalize(self.weight))\n","        sine = torch.sqrt((1.0 - torch.pow(cosine, 2)).clamp(0, 1))\n","        phi = cosine * self.cos_m - sine * self.sin_m\n","        if self.easy_margin:\n","            phi = torch.where(cosine > 0, phi, cosine)\n","        else:\n","            phi = torch.where(cosine > self.th, phi, cosine - self.mm)\n","        # --------------------------- convert label to one-hot ---------------------------\n","        # one_hot = torch.zeros(cosine.size(), requires_grad=True, device='cuda')\n","        one_hot = torch.zeros(cosine.size(), device='cuda')\n","        one_hot.scatter_(1, label.view(-1, 1).long(), 1)\n","        # -------------torch.where(out_i = {x_i if condition_i else y_i) -------------\n","        output = (one_hot * phi) + ((1.0 - one_hot) * cosine)  # you can use torch.where if your torch.__version__ is 0.4\n","        output *= self.s\n","        # print(output)\n","\n","        return output\n","\n","\n","class AddMarginProduct(nn.Module):\n","    r\"\"\"Implement of large margin cosine distance: :\n","    Args:\n","        in_features: size of each input sample\n","        out_features: size of each output sample\n","        s: norm of input feature\n","        m: margin\n","        cos(theta) - m\n","    \"\"\"\n","\n","    def __init__(self, in_features, out_features, s=30.0, m=0.40):\n","        super(AddMarginProduct, self).__init__()\n","        self.in_features = in_features\n","        self.out_features = out_features\n","        self.s = s\n","        self.m = m\n","        self.weight = Parameter(torch.FloatTensor(out_features, in_features))\n","        nn.init.xavier_uniform_(self.weight)\n","\n","    def forward(self, input, label):\n","        # --------------------------- cos(theta) & phi(theta) ---------------------------\n","        cosine = F.linear(F.normalize(input), F.normalize(self.weight))\n","        phi = cosine - self.m\n","        # --------------------------- convert label to one-hot ---------------------------\n","        one_hot = torch.zeros(cosine.size(), device='cuda')\n","        # one_hot = one_hot.cuda() if cosine.is_cuda else one_hot\n","        one_hot.scatter_(1, label.view(-1, 1).long(), 1)\n","        # -------------torch.where(out_i = {x_i if condition_i else y_i) -------------\n","        output = (one_hot * phi) + ((1.0 - one_hot) * cosine)  # you can use torch.where if your torch.__version__ is 0.4\n","        output *= self.s\n","        # print(output)\n","\n","        return output\n","\n","    def __repr__(self):\n","        return self.__class__.__name__ + '(' \\\n","               + 'in_features=' + str(self.in_features) \\\n","               + ', out_features=' + str(self.out_features) \\\n","               + ', s=' + str(self.s) \\\n","               + ', m=' + str(self.m) + ')'\n","\n","\n","class SphereProduct(nn.Module):\n","    r\"\"\"Implement of large margin cosine distance: :\n","    Args:\n","        in_features: size of each input sample\n","        out_features: size of each output sample\n","        m: margin\n","        cos(m*theta)\n","    \"\"\"\n","    def __init__(self, in_features, out_features, m=4):\n","        super(SphereProduct, self).__init__()\n","        self.in_features = in_features\n","        self.out_features = out_features\n","        self.m = m\n","        self.base = 1000.0\n","        self.gamma = 0.12\n","        self.power = 1\n","        self.LambdaMin = 5.0\n","        self.iter = 0\n","        self.weight = Parameter(torch.FloatTensor(out_features, in_features))\n","        nn.init.xavier_uniform(self.weight)\n","\n","        # duplication formula\n","        self.mlambda = [\n","            lambda x: x ** 0,\n","            lambda x: x ** 1,\n","            lambda x: 2 * x ** 2 - 1,\n","            lambda x: 4 * x ** 3 - 3 * x,\n","            lambda x: 8 * x ** 4 - 8 * x ** 2 + 1,\n","            lambda x: 16 * x ** 5 - 20 * x ** 3 + 5 * x\n","        ]\n","\n","    def forward(self, input, label):\n","        # lambda = max(lambda_min,base*(1+gamma*iteration)^(-power))\n","        self.iter += 1\n","        self.lamb = max(self.LambdaMin, self.base * (1 + self.gamma * self.iter) ** (-1 * self.power))\n","\n","        # --------------------------- cos(theta) & phi(theta) ---------------------------\n","        cos_theta = F.linear(F.normalize(input), F.normalize(self.weight))\n","        cos_theta = cos_theta.clamp(-1, 1)\n","        cos_m_theta = self.mlambda[self.m](cos_theta)\n","        theta = cos_theta.data.acos()\n","        k = (self.m * theta / 3.14159265).floor()\n","        phi_theta = ((-1.0) ** k) * cos_m_theta - 2 * k\n","        NormOfFeature = torch.norm(input, 2, 1)\n","\n","        # --------------------------- convert label to one-hot ---------------------------\n","        one_hot = torch.zeros(cos_theta.size())\n","        one_hot = one_hot.cuda() if cos_theta.is_cuda else one_hot\n","        one_hot.scatter_(1, label.view(-1, 1), 1)\n","\n","        # --------------------------- Calculate output ---------------------------\n","        output = (one_hot * (phi_theta - cos_theta) / (1 + self.lamb)) + cos_theta\n","        output *= NormOfFeature.view(-1, 1)\n","\n","        return output\n","\n","    def __repr__(self):\n","        return self.__class__.__name__ + '(' \\\n","               + 'in_features=' + str(self.in_features) \\\n","               + ', out_features=' + str(self.out_features) \\\n","               + ', m=' + str(self.m) + ')'"]}]}